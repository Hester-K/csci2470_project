{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c79b813f"
      },
      "source": [
        "## Environment Setup and Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71f1b144"
      },
      "source": [
        "pip -q install torch torchvision transformers openai requests Pillow numpy scikit-learn matplotlib seaborn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbdwC-Yf_fC6",
        "outputId": "330ca654-0314-4795-e25c-49ca5c545bc0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "dataset_url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
        "dataset_path = \"/content/drive/MyDrive/CSCI2470/final/stanford_dogs_dataset\"\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "    print(f\"Downloading Stanford Dogs dataset from {dataset_url}...\")\n",
        "    response = requests.get(dataset_url, stream=True)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "    tar_file_path = os.path.join(dataset_path, \"images.tar\")\n",
        "    with open(tar_file_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "    print(\"Download complete. Extracting...\")\n",
        "\n",
        "    with tarfile.open(tar_file_path, 'r') as tar:\n",
        "        tar.extractall(path=dataset_path)\n",
        "    print(\"Extraction complete.\")\n",
        "    os.remove(tar_file_path) # Clean up the tar file after extraction\n",
        "else:\n",
        "    print(\"Stanford Dogs dataset already downloaded and extracted.\")\n",
        "\n",
        "images_root = os.path.join(dataset_path, 'Images')\n",
        "if not os.path.exists(images_root):\n",
        "    print(f\"Warning: 'Images' subdirectory not found in {dataset_path}. Please check dataset structure.\")\n",
        "else:\n",
        "    print(f\"Stanford Dogs images root directory: {images_root}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1RvhnG7tCeo",
        "outputId": "3785389b-bdbf-4591-df7a-874003b5fec4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stanford Dogs dataset already downloaded and extracted.\n",
            "Stanford Dogs images root directory: /content/drive/MyDrive/CSCI2470/final/stanford_dogs_dataset/Images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fa5d758"
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class StanfordDogsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {}\n",
        "        self.idx_to_class = {}\n",
        "\n",
        "        # Collect image paths and assign labels\n",
        "        for i, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
        "            if not class_name.startswith('n'): # Skip non-breed directories\n",
        "                continue\n",
        "            self.class_to_idx[class_name] = i\n",
        "            self.idx_to_class[i] = class_name\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            for img_name in os.listdir(class_path):\n",
        "                self.image_paths.append(os.path.join(class_path, img_name))\n",
        "                self.labels.append(i)\n",
        "\n",
        "        print(f\"Found {len(self.image_paths)} images belonging to {len(self.class_to_idx)} classes.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "clip_model.eval()\n",
        "print(\"CLIP ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud8WicYo-Qgy",
        "outputId": "04f697bd-a8d9-4741-ba60-b45a1f3b2b58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = StanfordDogsDataset(root_dir=images_root, transform=None)\n",
        "\n",
        "emb_dir = os.path.join(dataset_path, 'embeddings')\n",
        "os.makedirs(emb_dir, exist_ok=True)\n",
        "emb_path = os.path.join(emb_dir, 'clip_embeddings.pt')\n",
        "\n",
        "if os.path.exists(emb_path):\n",
        "    print(f\"Embedding file {emb_path} already exists. Skipping generation.\")\n",
        "    data = torch.load(emb_path)\n",
        "    embeddings = data[\"embeddings\"]\n",
        "    labels = data[\"labels\"]\n",
        "else:\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        collate_fn=lambda batch: batch  # list of (image, label)\n",
        "    )\n",
        "\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            images, labels_batch = zip(*batch)\n",
        "            inputs = processor(images=list(images), return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            emb = clip_model.get_image_features(**inputs)\n",
        "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            all_embeddings.append(emb.cpu())\n",
        "            all_labels.append(torch.tensor(labels_batch))\n",
        "\n",
        "    embeddings = torch.cat(all_embeddings)\n",
        "    labels = torch.cat(all_labels)\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            \"embeddings\": embeddings,\n",
        "            \"labels\": labels\n",
        "        },\n",
        "        emb_path\n",
        "    )\n",
        "    print(f\"Saved embeddings to {emb_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-lJ7JkSBaoY",
        "outputId": "40891a63-f852-432c-bb93-9c898c6d4003"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20580 images belonging to 120 classes.\n",
            "Embedding file /content/drive/MyDrive/CSCI2470/final/stanford_dogs_dataset/embeddings/clip_embeddings.pt already exists. Skipping generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class StanfordDogsEmbeddingDataset(Dataset):\n",
        "    def __init__(self, emb_path):\n",
        "        data = torch.load(emb_path)\n",
        "        self.embeddings = data[\"embeddings\"]\n",
        "        self.labels = data[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "-vTXvm2mDzde"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# embedding Dataset\n",
        "full_dataset = StanfordDogsEmbeddingDataset(emb_path)\n",
        "\n",
        "all_labels = [full_dataset[i][1].item() for i in range(len(full_dataset))]\n",
        "all_indices = np.arange(len(full_dataset))\n",
        "\n",
        "use_full_data = True\n",
        "\n",
        "if use_full_data:\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        all_indices,\n",
        "        train_size=0.7,\n",
        "        stratify=all_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    temp_labels = [all_labels[i] for i in temp_idx]\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=temp_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    # 10% samples\n",
        "    sample_indices, _ = train_test_split(\n",
        "        all_indices,\n",
        "        train_size=0.1,\n",
        "        stratify=all_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    sample_labels = [all_labels[i] for i in sample_indices]\n",
        "\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        sample_indices,\n",
        "        train_size=0.7,\n",
        "        stratify=sample_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    temp_labels = [all_labels[i] for i in temp_idx]\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,\n",
        "        stratify=temp_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset = Subset(full_dataset, val_idx)\n",
        "test_dataset = Subset(full_dataset, test_idx)\n",
        "\n",
        "print(f\"Using full data: {use_full_data}\")\n",
        "print(\n",
        "    \"train =\", len(train_dataset),\n",
        "    \"val =\", len(val_dataset),\n",
        "    \"test =\", len(test_dataset)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRe4rtoeECKR",
        "outputId": "e8613299-8e93-4430-c4ef-c5eaf7e0810e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using full data: True\n",
            "train = 14405 val = 3087 test = 3088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders created successfully.\")\n",
        "print(f\"Train DataLoader has {len(train_dataloader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_dataloader)} batches.\")\n",
        "print(f\"Test DataLoader has {len(test_dataloader)} batches.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn06aj4REWja",
        "outputId": "e86a850f-ade7-4906-ea83-943ed5e845b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders created successfully.\n",
            "Train DataLoader has 226 batches.\n",
            "Validation DataLoader has 49 batches.\n",
            "Test DataLoader has 49 batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load LLM generated concepts & concept matrix"
      ],
      "metadata": {
        "id": "rP2XzPDJQbob"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aaafcb7",
        "outputId": "33ce8a34-e85d-4435-aede-682ec604280f"
      },
      "source": [
        "import json\n",
        "\n",
        "# Extract unique dog breed names\n",
        "dog_breed_names = [name.split('-')[-1].replace('_', ' ') for name in dataset.class_to_idx.keys()]\n",
        "dog_breed_class_ids = list(dataset.class_to_idx.values())\n",
        "\n",
        "# Create a mapping from cleaned breed name to class ID\n",
        "breed_name_to_class_id = {name.split('-')[-1].replace('_', ' '): class_id for name, class_id in dataset.class_to_idx.items()}\n",
        "class_id_to_breed_name = {v: k.split('-')[-1].replace('_', ' ') for k, v in dataset.class_to_idx.items()}\n",
        "\n",
        "print(f\"Extracted {len(dog_breed_names)} unique dog breed names.\")\n",
        "print(f\"First 5 breed names: {dog_breed_names[:5]}\")\n",
        "\n",
        "dog_breed_names_path = os.path.join(dataset_path, 'dog_breed_names.txt')\n",
        "\n",
        "if not os.path.exists(dog_breed_names_path):\n",
        "    with open(dog_breed_names_path, \"w\") as f:\n",
        "        json.dump(dog_breed_names, f)\n",
        "    print(f\"Saved {dog_breed_names_path}\")\n",
        "else:\n",
        "    print(f\"{dog_breed_names_path} already exists, skipping save.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 120 unique dog breed names.\n",
            "First 5 breed names: ['Chihuahua', 'Japanese spaniel', 'Maltese dog', 'Pekinese', 'Tzu']\n",
            "/content/drive/MyDrive/CSCI2470/final/stanford_dogs_dataset/dog_breed_names.txt already exists, skipping save.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_concepts = ['long coat', 'short coat', 'wir y coat', 'curly coat', 'double coat', 'single coat', 'beard', 'mustache', 'facial wrinkles', 'bushy eyebrows', 'ears erect', 'ears droopy', 'tail curled', 'tail docked', 'tail carried high', 'tail carried low', 'muzzle long', 'muzzle short', 'facial mask', 'neck ruff', 'feathering on legs', 'dewclaws present', 'dewclaws absent', 'webbed feet', 'brindle pattern', 'merle pattern', 'spotted coat', 'solid coat', 'jowls present']"
      ],
      "metadata": {
        "id": "o9sOZiEf8KiP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_matrix_path = '/content/drive/MyDrive/CSCI2470/final/outputs/concept_matrix.json'\n",
        "\n",
        "with open(concept_matrix_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    concept_matrix = json.load(f)"
      ],
      "metadata": {
        "id": "BuOJSON-axxt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline CLIP Linear Probing"
      ],
      "metadata": {
        "id": "lh1UatNgNmdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CLIPLinearProbe(nn.Module):\n",
        "    def __init__(self, clip_embed_dim=512, num_classes=120):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(clip_embed_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def train_clip_baseline(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for feats, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            feats = feats.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(feats)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * len(feats)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_labels = torch.cat(all_labels)\n",
        "        acc = (all_preds == all_labels).float().mean().item()\n",
        "        print(f\"[Train] Epoch {epoch+1} Loss: {epoch_loss/len(train_loader.dataset):.4f}, Top-1 Acc: {acc:.4f}\")\n",
        "\n",
        "        if epoch + 1 % 5 == 0:\n",
        "            val_top1, val_top5, val_loss = evaluate_clip_baseline(model, val_loader, device)\n",
        "            print(f\"[Val] Epoch {epoch+1} Loss: {val_loss:.4f}, Top-1: {val_top1:.4f}, Top-5: {val_top5:.4f}\")\n",
        "\n",
        "def evaluate_clip_baseline(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0\n",
        "    total_top1_correct = 0\n",
        "    total_top5_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for feats, labels in dataloader:\n",
        "            feats = feats.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(feats)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * len(feats)\n",
        "            total_samples += len(feats)\n",
        "\n",
        "            # Top-1\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            total_top1_correct += (preds == labels).sum().item()\n",
        "\n",
        "            # Top-5\n",
        "            top5_preds = torch.topk(logits, k=5, dim=1).indices\n",
        "            for i in range(len(labels)):\n",
        "                if labels[i] in top5_preds[i]:\n",
        "                    total_top5_correct += 1\n",
        "\n",
        "    top1_acc = total_top1_correct / total_samples\n",
        "    top5_acc = total_top5_correct / total_samples\n",
        "    avg_loss = total_loss / total_samples\n",
        "\n",
        "    return top1_acc, top5_acc, avg_loss\n"
      ],
      "metadata": {
        "id": "JaCX9jG2NpQa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = CLIPLinearProbe(clip_embed_dim=512, num_classes=120)\n",
        "\n",
        "train_clip_baseline(baseline_model, train_dataloader, val_loader=val_dataloader, num_epochs=25, lr=1e-3)\n",
        "\n",
        "test_top1, test_top5, test_loss = evaluate_clip_baseline(baseline_model, test_dataloader)\n",
        "print(f\"[Test] Top-1: {test_top1:.4f}, Top-5: {test_top5:.4f}, Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8hVooJuN2ge",
        "outputId": "c4581c1e-d063-4b6a-ad53-de2f686dc3f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25: 100%|██████████| 226/226 [00:01<00:00, 181.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 1 Loss: 4.6341, Top-1 Acc: 0.1377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25: 100%|██████████| 226/226 [00:00<00:00, 374.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 2 Loss: 4.3419, Top-1 Acc: 0.3243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25: 100%|██████████| 226/226 [00:00<00:00, 314.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 3 Loss: 4.0920, Top-1 Acc: 0.4208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25: 100%|██████████| 226/226 [00:00<00:00, 355.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 4 Loss: 3.8753, Top-1 Acc: 0.4875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25: 100%|██████████| 226/226 [00:00<00:00, 383.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 5 Loss: 3.6874, Top-1 Acc: 0.5111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25: 100%|██████████| 226/226 [00:00<00:00, 380.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 6 Loss: 3.5256, Top-1 Acc: 0.5588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25: 100%|██████████| 226/226 [00:00<00:00, 361.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 7 Loss: 3.3854, Top-1 Acc: 0.5707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25: 100%|██████████| 226/226 [00:00<00:00, 370.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 8 Loss: 3.2641, Top-1 Acc: 0.5858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/25: 100%|██████████| 226/226 [00:00<00:00, 325.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 9 Loss: 3.1597, Top-1 Acc: 0.6021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/25: 100%|██████████| 226/226 [00:00<00:00, 283.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 10 Loss: 3.0691, Top-1 Acc: 0.6126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/25: 100%|██████████| 226/226 [00:00<00:00, 319.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 11 Loss: 2.9902, Top-1 Acc: 0.6230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/25: 100%|██████████| 226/226 [00:00<00:00, 562.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 12 Loss: 2.9216, Top-1 Acc: 0.6287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/25: 100%|██████████| 226/226 [00:00<00:00, 530.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 13 Loss: 2.8617, Top-1 Acc: 0.6377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/25: 100%|██████████| 226/226 [00:00<00:00, 545.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 14 Loss: 2.8097, Top-1 Acc: 0.6372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/25: 100%|██████████| 226/226 [00:00<00:00, 528.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 15 Loss: 2.7642, Top-1 Acc: 0.6482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/25: 100%|██████████| 226/226 [00:00<00:00, 546.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 16 Loss: 2.7244, Top-1 Acc: 0.6494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/25: 100%|██████████| 226/226 [00:00<00:00, 533.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 17 Loss: 2.6889, Top-1 Acc: 0.6562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/25: 100%|██████████| 226/226 [00:00<00:00, 523.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 18 Loss: 2.6582, Top-1 Acc: 0.6580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/25: 100%|██████████| 226/226 [00:00<00:00, 494.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 19 Loss: 2.6308, Top-1 Acc: 0.6644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/25: 100%|██████████| 226/226 [00:00<00:00, 452.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 20 Loss: 2.6065, Top-1 Acc: 0.6669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/25: 100%|██████████| 226/226 [00:00<00:00, 444.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 21 Loss: 2.5854, Top-1 Acc: 0.6677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/25: 100%|██████████| 226/226 [00:00<00:00, 452.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 22 Loss: 2.5662, Top-1 Acc: 0.6666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/25: 100%|██████████| 226/226 [00:00<00:00, 475.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 23 Loss: 2.5492, Top-1 Acc: 0.6748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/25: 100%|██████████| 226/226 [00:00<00:00, 435.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 24 Loss: 2.5341, Top-1 Acc: 0.6753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/25: 100%|██████████| 226/226 [00:00<00:00, 459.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 25 Loss: 2.5207, Top-1 Acc: 0.6771\n",
            "[Test] Top-1: 0.6218, Top-5: 0.9258, Loss: 2.5877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBM Train & Test"
      ],
      "metadata": {
        "id": "e7I9zGpIwcbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ConceptPredictor(nn.Module):\n",
        "    def __init__(self, in_dim=512, num_concepts=20):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, num_concepts)\n",
        "        )\n",
        "    def forward(self, feats):\n",
        "        return self.mlp(feats)\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "    def __init__(self, num_concepts=20, num_classes=120):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(num_concepts, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, c):\n",
        "        return self.mlp(c)\n",
        "\n",
        "class CBM(nn.Module):\n",
        "    def __init__(self, num_concepts, num_classes):\n",
        "        super().__init__()\n",
        "        self.concept_pred = ConceptPredictor(512, num_concepts)\n",
        "        self.classifier = LabelPredictor(num_concepts, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        concepts = self.concept_pred(x)\n",
        "        logits   = self.classifier(concepts)\n",
        "        return concepts, logits\n",
        "\n",
        "cbm_model = CBM(num_concepts=len(llm_concepts), num_classes=len(dog_breed_names)).to(device)\n"
      ],
      "metadata": {
        "id": "6-QrTayYxSEq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cbm(cbm_model, train_loader, val_loader, concept_matrix, num_epochs_concept=15, num_epochs_label=10, lr=1e-3):\n",
        "    cbm_model.to(device)\n",
        "\n",
        "    # Concept Predictor\n",
        "    optimizer_c = torch.optim.Adam(cbm_model.concept_pred.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    criterion_c = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs_concept):\n",
        "        cbm_model.train()\n",
        "        epoch_loss = 0\n",
        "        for feats, class_labels in train_loader:\n",
        "            feats = feats.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "\n",
        "            # construct concept labels from LLM concept matrix\n",
        "            concept_labels = torch.tensor(\n",
        "                [\n",
        "                    [concept_matrix[class_id_to_breed_name[cid.item()]][c] for c in llm_concepts]\n",
        "                    for cid in class_labels\n",
        "                ],\n",
        "                dtype=torch.float32,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            optimizer_c.zero_grad()\n",
        "            concepts, _ = cbm_model(feats)\n",
        "            loss = criterion_c(concepts, concept_labels)\n",
        "            loss.backward()\n",
        "            optimizer_c.step()\n",
        "            epoch_loss += loss.item() * len(feats)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "          print(f\"[Concept] Epoch {epoch+1}/{num_epochs_concept} Loss: {epoch_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "    # Freeze Concept Predictor\n",
        "    for param in cbm_model.concept_pred.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Label Predictor\n",
        "    optimizer_l = torch.optim.Adam(cbm_model.classifier.parameters(), lr=lr)\n",
        "    criterion_l = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs_label):\n",
        "        cbm_model.train()\n",
        "        epoch_loss = 0\n",
        "        for feats, class_labels in train_loader:\n",
        "            feats = feats.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "\n",
        "            optimizer_l.zero_grad()\n",
        "            concepts, logits = cbm_model(feats)\n",
        "            loss = criterion_l(logits, class_labels)\n",
        "            loss.backward()\n",
        "            optimizer_l.step()\n",
        "            epoch_loss += loss.item() * len(feats)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "          print(f\"[Label] Epoch {epoch+1}/{num_epochs_label} Loss: {epoch_loss/len(train_loader.dataset):.4f}\")\n"
      ],
      "metadata": {
        "id": "WRTxlishygEF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_cbm(cbm_model, dataloader, concept_matrix=None):\n",
        "    cbm_model.eval()\n",
        "    criterion_l = nn.CrossEntropyLoss()\n",
        "    criterion_c = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    total_loss_label = 0.0\n",
        "    total_loss_concept = 0.0\n",
        "    total_correct_top1 = 0\n",
        "    total_correct_top5 = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for feats, class_labels in dataloader:\n",
        "            feats = feats.to(device)\n",
        "            class_labels = class_labels.to(device)\n",
        "            batch_size = class_labels.size(0)\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # ---------- Concept Predictor ----------\n",
        "            concepts_pred, logits = cbm_model(feats)\n",
        "\n",
        "            # Compute concept loss if concept_matrix given\n",
        "            if concept_matrix is not None:\n",
        "                concept_labels = torch.tensor(\n",
        "                    [\n",
        "                        [concept_matrix[class_id_to_breed_name[cid.item()]][c] for c in llm_concepts]\n",
        "                        for cid in class_labels\n",
        "                    ],\n",
        "                    dtype=torch.float32,\n",
        "                    device=device\n",
        "                )\n",
        "                loss_c = criterion_c(concepts_pred, concept_labels)\n",
        "                total_loss_concept += loss_c.item() * batch_size\n",
        "\n",
        "            # ---------- Label Predictor ----------\n",
        "            loss_l = criterion_l(logits, class_labels)\n",
        "            total_loss_label += loss_l.item() * batch_size\n",
        "\n",
        "            # ---------- Predictions ----------\n",
        "            # Top-1\n",
        "            preds_top1 = torch.argmax(logits, dim=1)\n",
        "            total_correct_top1 += (preds_top1 == class_labels).sum().item()\n",
        "            # Top-5\n",
        "            preds_top5 = torch.topk(logits, k=5, dim=1)[1]  # [B, 5]\n",
        "            total_correct_top5 += (preds_top5 == class_labels.view(-1, 1)).any(dim=1).sum().item()\n",
        "\n",
        "    accuracy_top1 = total_correct_top1 / total_samples\n",
        "    accuracy_top5 = total_correct_top5 / total_samples\n",
        "    avg_loss_label = total_loss_label / total_samples\n",
        "    avg_loss_concept = total_loss_concept / total_samples if concept_matrix is not None else None\n",
        "\n",
        "    return accuracy_top1, accuracy_top5, avg_loss_label, avg_loss_concept\n"
      ],
      "metadata": {
        "id": "bRhY4wkXH_GF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cbm(cbm_model, train_dataloader, val_dataloader, concept_matrix, num_epochs_concept=20, num_epochs_label=35)"
      ],
      "metadata": {
        "id": "2eFw_NfoFfrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20f7c14d-4cf6-4007-9733-cda9cd43ff27"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Concept] Epoch 5/20 Loss: 0.3282\n",
            "[Concept] Epoch 10/20 Loss: 0.3136\n",
            "[Concept] Epoch 15/20 Loss: 0.3069\n",
            "[Concept] Epoch 20/20 Loss: 0.3024\n",
            "[Label] Epoch 5/35 Loss: 1.4371\n",
            "[Label] Epoch 10/35 Loss: 1.2695\n",
            "[Label] Epoch 15/35 Loss: 1.1836\n",
            "[Label] Epoch 20/35 Loss: 1.1182\n",
            "[Label] Epoch 25/35 Loss: 1.0723\n",
            "[Label] Epoch 30/35 Loss: 1.0468\n",
            "[Label] Epoch 35/35 Loss: 1.0015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Training set ----------\n",
        "train_top1, train_top5, train_loss_label, train_loss_concept = evaluate_cbm(\n",
        "    cbm_model, train_dataloader, concept_matrix\n",
        ")\n",
        "print(f\"[Train] Top-1: {train_top1:.4f}, Top-5: {train_top5:.4f}, Label Loss: {train_loss_label:.4f}, Concept Loss: {train_loss_concept:.4f}\")\n",
        "\n",
        "# ---------- Test set ----------\n",
        "test_top1, test_top5, test_loss_label, test_loss_concept = evaluate_cbm(\n",
        "    cbm_model, test_dataloader, concept_matrix\n",
        ")\n",
        "print(f\"[Test] Top-1: {test_top1:.4f}, Top-5: {test_top5:.4f}, Label Loss: {test_loss_label:.4f}, Concept Loss: {test_loss_concept:.4f}\")\n",
        "\n",
        "# ---------- Validation ----------\n",
        "val_top1, val_top5, val_loss_label, val_loss_concept = evaluate_cbm(\n",
        "    cbm_model, val_dataloader, concept_matrix\n",
        ")\n",
        "print(f\"[Test] Top-1: {val_top1:.4f}, Top-5: {val_top5:.4f}, Label Loss: {val_loss_label:.4f}, Concept Loss: {val_loss_concept:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv7iiTr3ICJQ",
        "outputId": "73264252-8ea2-4a60-c27a-545e3ba24865"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Top-1: 0.6816, Top-5: 0.9515, Label Loss: 0.9890, Concept Loss: 0.3001\n",
            "[Test] Top-1: 0.6205, Top-5: 0.9317, Label Loss: 1.2203, Concept Loss: 0.3039\n",
            "[Test] Top-1: 0.6168, Top-5: 0.9252, Label Loss: 1.2576, Concept Loss: 0.3047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label-free CBM\n",
        "\n",
        "Reference:\n",
        "\n",
        "https://github.com/Trustworthy-ML-Lab/Label-free-CBM\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/2304.06129"
      ],
      "metadata": {
        "id": "H9y_WyRL4RP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concepts_path = '/content/drive/MyDrive/CSCI2470/final/outputs/label_free_concepts.json'\n",
        "\n",
        "with open(concepts_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    breed_to_concepts = json.load(f)\n",
        "\n",
        "all_concepts = []\n",
        "for concepts in breed_to_concepts.values():\n",
        "    all_concepts.extend(concepts)\n",
        "\n",
        "concepts_inputs = processor(text=all_concepts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = clip_model.get_text_features(**concepts_inputs)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "iOi1p0XlCGbh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class ConceptProj(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def train_cbm_from_embeddings(train_loader, val_loader, text_features,\n",
        "                              device='cuda', proj_steps=500, interpret_cutoff=0.2,\n",
        "                              lr=1e-3, num_epochs_cls=20, batch_size_cls=64):\n",
        "\n",
        "    all_feats, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for feats, labels in train_loader:\n",
        "            all_feats.append(feats.to(device))\n",
        "            all_labels.append(labels.to(device))\n",
        "\n",
        "    target_features = torch.cat(all_feats)  # [N, image_emb_dim]\n",
        "    labels = torch.cat(all_labels)           # [N]\n",
        "\n",
        "    # projection layer\n",
        "    text_features = text_features.to(device)\n",
        "    text_features = text_features / text_features.norm(dim=1, keepdim=True)  # normalize\n",
        "    proj_layer = ConceptProj(target_features.size(1), text_features.size(1)).to(device)\n",
        "    optimizer = torch.optim.Adam(proj_layer.parameters(), lr=lr)\n",
        "\n",
        "    for step in range(proj_steps):\n",
        "        optimizer.zero_grad()\n",
        "        out = proj_layer(target_features)              # [N, text_emb_dim]\n",
        "        # print(\"Projection output mean/std:\", out.mean().item(), out.std().item())\n",
        "        out = out / out.norm(dim=1, keepdim=True)      # normalize\n",
        "        sim_matrix = out @ text_features.T            # [N, num_concepts]\n",
        "        # print(\"Initial sim_matrix mean/std:\", sim_matrix.mean().item(), sim_matrix.std().item())\n",
        "        loss = -sim_matrix.mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Proj Step {step}, Loss {loss.item():.4f}\")\n",
        "\n",
        "    # concept features\n",
        "    with torch.no_grad():\n",
        "        proj_out = proj_layer(target_features)\n",
        "        proj_out = proj_out / proj_out.norm(dim=1, keepdim=True)\n",
        "        concept_feats = proj_out @ text_features.T   # [N, num_concepts]\n",
        "\n",
        "        # interpretability cutoff\n",
        "        sim_per_concept = concept_feats.mean(dim=0)  # [num_concepts]\n",
        "        active_idx = sim_per_concept > interpret_cutoff\n",
        "        print(f\"Keeping {active_idx.sum().item()}/{text_features.size(0)} concepts\")\n",
        "\n",
        "        concept_feats = concept_feats[:, active_idx]\n",
        "\n",
        "    mean = concept_feats.mean(dim=0, keepdim=True)\n",
        "    std  = concept_feats.std(dim=0, keepdim=True)\n",
        "    concept_feats = (concept_feats - mean) / (std + 1e-8)\n",
        "\n",
        "    # label predictor\n",
        "    classifier = LinearClassifier(concept_feats.size(1), labels.max().item()+1).to(device)\n",
        "    optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    dataset = TensorDataset(concept_feats, labels)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size_cls, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs_cls):\n",
        "        classifier.train()\n",
        "        total_loss = 0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer_cls.zero_grad()\n",
        "            logits = classifier(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer_cls.step()\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "          print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataset):.4f}\")\n",
        "\n",
        "    return proj_layer, classifier, mean, std, active_idx\n"
      ],
      "metadata": {
        "id": "eLN8jC9CFPPj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cbm_topk(test_loader, proj_layer, classifier, text_features, mean, std, active_idx, device='cuda'):\n",
        "    proj_layer.eval()\n",
        "    classifier.eval()\n",
        "    text_features = text_features.to(device)\n",
        "    text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "    all_labels = []\n",
        "    top1_correct = 0\n",
        "    top5_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for feats, labels in test_loader:\n",
        "            feats = feats.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = feats.size(0)\n",
        "\n",
        "            proj_out = proj_layer(feats)\n",
        "            proj_out = proj_out / proj_out.norm(dim=1, keepdim=True)\n",
        "\n",
        "            # generate concept features\n",
        "            concept_feats = proj_out @ text_features.T\n",
        "            concept_feats = concept_feats[:, active_idx]\n",
        "            concept_feats = (concept_feats - mean) / (std + 1e-8)\n",
        "\n",
        "            logits = classifier(concept_feats)\n",
        "\n",
        "            # top k predictions\n",
        "            top5_preds = torch.topk(logits, k=5, dim=1).indices\n",
        "            top1_preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            top1_correct += (top1_preds == labels).sum().item()\n",
        "            top5_correct += sum([1 if labels[i] in top5_preds[i] else 0 for i in range(batch_size)])\n",
        "            total += batch_size\n",
        "\n",
        "    top1_acc = top1_correct / total\n",
        "    top5_acc = top5_correct / total\n",
        "    print(f\"Top-1 Accuracy: {top1_acc*100:.2f}%\")\n",
        "    print(f\"Top-5 Accuracy: {top5_acc*100:.2f}%\")\n",
        "    return top1_acc, top5_acc\n"
      ],
      "metadata": {
        "id": "bpZnkuiSjZmj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proj_layer, classifier, mean, std, active_idx = train_cbm_from_embeddings(\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    text_features=text_features,\n",
        "    device=device,\n",
        "    proj_steps=200,\n",
        "    interpret_cutoff=0.85,\n",
        "    lr=3e-3,\n",
        "    num_epochs_cls=100,\n",
        "    batch_size_cls=64\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyt0NLCpG8by",
        "outputId": "6b47d97b-e489-49eb-c3c3-7ba3f2e1a484"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proj Step 0, Loss 0.0348\n",
            "Proj Step 50, Loss -0.8707\n",
            "Proj Step 100, Loss -0.8712\n",
            "Proj Step 150, Loss -0.8712\n",
            "Keeping 425/585 concepts\n",
            "Epoch 5, Loss: 2.2841\n",
            "Epoch 10, Loss: 1.9162\n",
            "Epoch 15, Loss: 1.7122\n",
            "Epoch 20, Loss: 1.5785\n",
            "Epoch 25, Loss: 1.4767\n",
            "Epoch 30, Loss: 1.4006\n",
            "Epoch 35, Loss: 1.3526\n",
            "Epoch 40, Loss: 1.2985\n",
            "Epoch 45, Loss: 1.2509\n",
            "Epoch 50, Loss: 1.2609\n",
            "Epoch 55, Loss: 1.2315\n",
            "Epoch 60, Loss: 1.1609\n",
            "Epoch 65, Loss: 1.1310\n",
            "Epoch 70, Loss: 1.1709\n",
            "Epoch 75, Loss: 1.0545\n",
            "Epoch 80, Loss: 1.0626\n",
            "Epoch 85, Loss: 1.0443\n",
            "Epoch 90, Loss: 1.0317\n",
            "Epoch 95, Loss: 1.0144\n",
            "Epoch 100, Loss: 1.0093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Training set ----------\n",
        "train_top1, train_top5 = test_cbm_topk(\n",
        "    test_loader=train_dataloader,\n",
        "    proj_layer=proj_layer,\n",
        "    classifier=classifier,\n",
        "    text_features=text_features,\n",
        "    mean=mean,\n",
        "    std=std,\n",
        "    active_idx=active_idx,\n",
        "    device=device\n",
        ")\n",
        "print(f\"[Train] Top-1: {train_top1:.4f}, Top-5: {train_top5:.4f}\")\n",
        "\n",
        "# ---------- Validation set ----------\n",
        "val_top1, val_top5 = test_cbm_topk(\n",
        "    test_loader=val_dataloader,\n",
        "    proj_layer=proj_layer,\n",
        "    classifier=classifier,\n",
        "    text_features=text_features,\n",
        "    mean=mean,\n",
        "    std=std,\n",
        "    active_idx=active_idx,\n",
        "    device=device\n",
        ")\n",
        "print(f\"[Val] Top-1: {val_top1:.4f}, Top-5: {val_top5:.4f}\")\n",
        "\n",
        "# ---------- Test set ----------\n",
        "test_top1, test_top5 = test_cbm_topk(\n",
        "    test_loader=test_dataloader,\n",
        "    proj_layer=proj_layer,\n",
        "    classifier=classifier,\n",
        "    text_features=text_features,\n",
        "    mean=mean,\n",
        "    std=std,\n",
        "    active_idx=active_idx,\n",
        "    device=device\n",
        ")\n",
        "print(f\"[Test] Top-1: {test_top1:.4f}, Top-5: {test_top5:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CthVf3J2jqao",
        "outputId": "0347979c-8c8a-405d-cb68-78d4c39fb0a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 74.99%\n",
            "Top-5 Accuracy: 94.95%\n",
            "[Train] Top-1: 0.7499, Top-5: 0.9495\n",
            "Top-1 Accuracy: 50.63%\n",
            "Top-5 Accuracy: 79.46%\n",
            "[Val] Top-1: 0.5063, Top-5: 0.7946\n",
            "Top-1 Accuracy: 51.10%\n",
            "Top-5 Accuracy: 80.89%\n",
            "[Test] Top-1: 0.5110, Top-5: 0.8089\n"
          ]
        }
      ]
    }
  ]
}